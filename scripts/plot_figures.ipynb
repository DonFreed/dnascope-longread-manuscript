{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot figures for the DNAscope LongRead paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "work_dir = \"/work\"\n",
    "dnascope_output_format = \"dnascope/{ref}/{sample}/{aln_params}/{subset}/calls.vcf.gz\"\n",
    "happy_merged_output = \"happy_merged/happy_extended_metrics.tsv\"\n",
    "config_file = \"config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import math\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file\n",
    "config = yaml.load(open(config_file), Loader=yaml.CLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_processed_samples():\n",
    "    \"\"\" A generator over the processed samples \"\"\"\n",
    "    d = {\n",
    "        \"ref\": \"hs38\", \"aln_params\": \"--preset HiFi -c 0 -y 70\"\n",
    "    }\n",
    "    \n",
    "    # All samples with full coverage\n",
    "    d[\"truthset\"] = \"v4.2.1\"\n",
    "    d[\"subset\"] = \"full\"\n",
    "    for sample in config[\"input\"][\"samples\"].keys():\n",
    "        d[\"sample\"] = sample\n",
    "        yield d.copy()\n",
    "    \n",
    "    # Downsamples of HG003\n",
    "    d[\"sample\"] = \"pFDA-truth_V2-PB-HG003\"\n",
    "    for subset in config[\"pipeline\"][\"hg003_subsets\"]:\n",
    "        d[\"subset\"] = subset\n",
    "        yield d.copy()\n",
    "    \n",
    "    # pFDA HG002 with CMRG truthset\n",
    "    d[\"truthset\"] = \"CMRGv1.00\"\n",
    "    d[\"sample\"] = \"pFDA-truth_V2-PB-HG002\"\n",
    "    d[\"subset\"] = \"full\"\n",
    "    yield d.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_row_values(col, _dict):\n",
    "    \"\"\" A small utility function for updating values in a pandas DataFrame using a dict to map values \"\"\"\n",
    "    def _update_row_values(row):\n",
    "        val = row[col]\n",
    "        if val not in _dict:\n",
    "            return val\n",
    "        else:\n",
    "            return _dict[row[col]]\n",
    "    return _update_row_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def happy_to_errors(df):\n",
    "    \"\"\" Convert hap.py rows to a table with errors for plotting \"\"\"\n",
    "    new_df = {\n",
    "        \"Errors\": [], \"Variant Type\": [], \"Error\": [],\n",
    "    }\n",
    "    for idx, row in df.iterrows():\n",
    "        error_types = (\"TRUTH.FN\", \"QUERY.FP\")\n",
    "        error_names = (\"False Negative\", \"False Positive\")\n",
    "        for error_type, name in zip(error_types, error_names):\n",
    "            new_df[\"Errors\"].append(row[error_type])\n",
    "            new_df[\"Variant Type\"].append(row[\"Type\"])\n",
    "            new_df[\"Error\"].append(name)\n",
    "    return pd.DataFrame(new_df)\n",
    "\n",
    "def happy_to_acc(df):\n",
    "    \"\"\" Convert hap.py rows to a table with accuracy metrics for plotting \"\"\"\n",
    "    new_df = {\n",
    "        \"Performance\": [], \"Variant Type\": [], \"Metric\": [],\n",
    "    }\n",
    "    for idx, row in df.iterrows():\n",
    "        metric_types = (\"METRIC.Recall\", \"METRIC.Precision\")\n",
    "        metric_names = (\"Recall\", \"Precision\")\n",
    "        for metric_type, name in zip(metric_types, metric_names):\n",
    "            new_df[\"Performance\"].append(row[metric_type])\n",
    "            new_df[\"Variant Type\"].append(row[\"Type\"])\n",
    "            new_df[\"Metric\"].append(name)\n",
    "    return pd.DataFrame(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the runtime for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_runtime_metrics = {}\n",
    "StageRuntime = namedtuple(\"StageRuntime\", [\"memory\", \"user\", \"sys\", \"real\"])\n",
    "runtime_stages = [\n",
    "    \"RepeatStat\", \"DNAscope-diploid\", \"DNAModelApply-diploid\", \"VariantPhaser\", \n",
    "    \"RepeatModel\", \"DNAscope-hap1\", \"DNAscope-hap2\", \"DNAModelApply-hap1\",\n",
    "    \"DNAModelApply-hap2\", \"DNAscopeHP-unphased\", \"DNAModelApply-unphased\",\n",
    "]\n",
    "for wildcards in iter_processed_samples():\n",
    "    # Skip the CMRG iteration - runtime is the same as v4.2.1\n",
    "    if wildcards[\"truthset\"] == \"CMRGv1.00\":\n",
    "        continue\n",
    "    \n",
    "    sample_metrics = {}\n",
    "    coverage_name = \"full\"\n",
    "    if wildcards[\"subset\"] != \"full\":\n",
    "        downsample_frac = float(f\"0.{wildcards['subset']}\")\n",
    "        downsample_target = int(downsample_frac * 36.0)\n",
    "        coverage_name = f\"{downsample_target}x\"\n",
    "    sample_name = f\"{wildcards['sample']}--{coverage_name}\"\n",
    "    \n",
    "    dnascope_vcf_fn = os.path.join(work_dir, dnascope_output_format.format(**wildcards))\n",
    "    stdout_fn, stderr_fn, benchmark_fn = (\n",
    "        dnascope_vcf_fn + \".stdout\", \n",
    "        dnascope_vcf_fn + \".stderr\", \n",
    "        dnascope_vcf_fn + \".benchmark.txt\",\n",
    "    )\n",
    "    \n",
    "    # Collect the memory usage from the \".benchmark.txt\" file. Runtime and io \n",
    "    # metrics will not be accurate from this file due to the input BAM being copied\n",
    "    # to a local SSD inside this rule\n",
    "    benchmark_metrics = pd.read_csv(benchmark_fn, sep='\\t', header=0)\n",
    "    sample_metrics[\"max_vms\"] = float(benchmark_metrics.at[0, \"max_vms\"])\n",
    "    \n",
    "    # Overall runtime is written to stdout\n",
    "    start_time, end_time = None, None\n",
    "    with open(stdout_fn) as stdout_fh:\n",
    "        for line in stdout_fh:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith(\"Start time: \"):\n",
    "                if start_time is not None:\n",
    "                    raise ValueError(f\"Found multiple start times in file, '{stdout_fn}'\")\n",
    "                start_time = int(line[12:])\n",
    "            if line.startswith(\"End time: \"):\n",
    "                if end_time is not None:\n",
    "                    raise ValueError(f\"Found multiple end times in file, '{stdout_fn}'\")\n",
    "                end_time = int(line[10:])\n",
    "    \n",
    "    if not start_time or not end_time:\n",
    "        raise ValueError(f\"Failed to find start or end time in file, '{stdout_fn}'\")\n",
    "    sample_metrics[\"overall_runtime\"] = end_time - start_time\n",
    "    \n",
    "    # Per-stage runtime is written to stderr in a pre-determined order\n",
    "    stage_metrics = []\n",
    "    with open(stderr_fn) as stderr_fh:\n",
    "        for line in stderr_fh:\n",
    "            if not line.startswith(\"overall: \"):\n",
    "                continue\n",
    "            line = line.rstrip().split(' ')\n",
    "            stage_metrics.append(\n",
    "                StageRuntime(\n",
    "                    int(line[1]),\n",
    "                    float(line[3]),\n",
    "                    float(line[5]),\n",
    "                    float(line[7]),\n",
    "                )\n",
    "            )\n",
    "    # Find runtime that is not in the per-stage runtime\n",
    "    accounted_runtime = sum([x.real for x in stage_metrics])\n",
    "    other_runtime = sample_metrics[\"overall_runtime\"] - accounted_runtime\n",
    "    \n",
    "    sample_metrics.update(dict(zip(runtime_stages, stage_metrics)))\n",
    "    sample_metrics[\"other_runtime\"] = other_runtime\n",
    "    sample_runtime_metrics[sample_name] = sample_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the aggregated hap.py metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_happy_fn = os.path.join(work_dir, happy_merged_output)\n",
    "accuracy_metrics = pd.read_csv(merged_happy_fn, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build rows for combined SNVs and INDELs\n",
    "sample_subset = {}\n",
    "for idx, row in accuracy_metrics.iterrows():\n",
    "    _subset_sample = (row[\"Subset\"], row[\"Sample\"])\n",
    "    \n",
    "    # initialize the sample/subset, if necessary\n",
    "    if _subset_sample not in sample_subset:\n",
    "        sample_subset[_subset_sample] = [0, 0, 0, 0]\n",
    "    \n",
    "    # Add the new data to the earlier data\n",
    "    row_data = list(row[[\"TRUTH.TOTAL\", \"TRUTH.TP\", \"TRUTH.FN\", \"QUERY.FP\"]])\n",
    "    sample_subset[_subset_sample] = [sum(x) for x in zip(sample_subset[_subset_sample], row_data)]\n",
    "\n",
    "# Build a new DF with SNV+INDEL\n",
    "new_df = {\n",
    "    \"Type\": [], \"Subset\": [], \"METRIC.Recall\": [], \"METRIC.Precision\": [],\n",
    "    \"METRIC.F1_Score\": [], \"TRUTH.TOTAL\": [], \"TRUTH.TP\": [], \"TRUTH.FN\": [],\n",
    "    \"QUERY.FP\": [], \"TOTAL.ERRORS\": [], \"Sample\": [],\n",
    "}\n",
    "for _subset_sample, metrics in sample_subset.items():\n",
    "    subset, sample = _subset_sample\n",
    "    total, tp, fn, fp = metrics\n",
    "    \n",
    "    new_df[\"Type\"].append(\"SNP+INDEL\")\n",
    "    new_df[\"Subset\"].append(subset)\n",
    "    new_df[\"Sample\"].append(sample)\n",
    "    \n",
    "    try:\n",
    "        recall = float(tp) / total\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0.0\n",
    "    \n",
    "    try:\n",
    "        precision = float(tp) / (tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0.0\n",
    "    \n",
    "    try:\n",
    "        f1_score = 2 * recall * precision / (recall + precision)\n",
    "    except ZeroDivisionError:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    new_df[\"METRIC.Recall\"].append(recall)\n",
    "    new_df[\"METRIC.Precision\"].append(precision)\n",
    "    new_df[\"METRIC.F1_Score\"].append(f1_score)\n",
    "    new_df[\"TRUTH.TOTAL\"].append(total)\n",
    "    new_df[\"TRUTH.TP\"].append(tp)\n",
    "    new_df[\"TRUTH.FN\"].append(fn)\n",
    "    new_df[\"QUERY.FP\"].append(fp)\n",
    "    new_df[\"TOTAL.ERRORS\"].append(fn + fp)\n",
    "\n",
    "new_df = pd.DataFrame(data=new_df)\n",
    "\n",
    "# Concat the DFs\n",
    "accuracy_metrics = pd.concat([accuracy_metrics, new_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate figures\n",
    "### Runtime by Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an empty dict with the columns\n",
    "runtime_df = {\"sample\":[]}\n",
    "plot_stages = runtime_stages + [\"other_runtime\"]\n",
    "for stage in plot_stages:\n",
    "    runtime_df[stage] = []\n",
    "\n",
    "# Add the data\n",
    "for sample, metrics in sample_runtime_metrics.items():\n",
    "    runtime_df[\"sample\"].append(sample)\n",
    "    for stage in plot_stages:\n",
    "        value = metrics[stage]\n",
    "        if isinstance(value, float):\n",
    "            runtime_df[stage].append(metrics[stage])\n",
    "        else:\n",
    "            runtime_df[stage].append(metrics[stage].real)\n",
    "runtime_df = pd.DataFrame(data=runtime_df)\n",
    "\n",
    "# Use better names\n",
    "remap_names = {\n",
    "    'pFDA-truth_V2-PB-HG003--5x': \"pFDA-HG3-5x\",\n",
    "    'pFDA-truth_V2-PB-HG003--7x': \"pFDA-HG3-7x\",\n",
    "    'pFDA-truth_V2-PB-HG003--8x': \"pFDA-HG3-8x\",\n",
    "    'pFDA-truth_V2-PB-HG003--9x': \"pFDA-HG3-9x\",\n",
    "    'pFDA-truth_V2-PB-HG003--10x': \"pFDA-HG3-10x\",\n",
    "    'pFDA-truth_V2-PB-HG003--11x': \"pFDA-HG3-11x\",\n",
    "    'pFDA-truth_V2-PB-HG003--13x': \"pFDA-HG3-13x\",\n",
    "    'pFDA-truth_V2-PB-HG003--15x': \"pFDA-HG3-15x\",\n",
    "    'pFDA-truth_V2-PB-HG003--17x': \"pFDA-HG3-17x\",\n",
    "    'pFDA-truth_V2-PB-HG003--20x': \"pFDA-HG3-20x\",\n",
    "    'pFDA-truth_V2-PB-HG003--25x': \"pFDA-HG3-25x\",\n",
    "    'pFDA-truth_V2-PB-HG003--30x': \"pFDA-HG3-30x\",\n",
    "    'pFDA-truth_V2-PB-HG003--full': \"pFDA-HG3-full\",\n",
    "    'pFDA-truth_V2-PB-HG004--full': \"pFDA-HG4-full\",\n",
    "    'pFDA-truth_V2-PB-HG002--full': \"pFDA-HG2-full\",\n",
    "    'HG002-chemV2.2-sample--full': \"chemV2.2-HG2-full\",\n",
    "}\n",
    "runtime_df[\"sample\"] = runtime_df.apply(update_row_values(\"sample\", remap_names), axis=1)\n",
    "\n",
    "# Skip samples not run under benchmark conditions\n",
    "skip_samples = [\n",
    "    \"pFDA-HG3-11x\",\n",
    "    \"pFDA-HG3-13x\",\n",
    "    \"pFDA-HG3-17x\",\n",
    "]\n",
    "runtime_df = runtime_df.loc[\n",
    "    (~runtime_df[\"sample\"].isin(skip_samples))\n",
    "]\n",
    "\n",
    "# Re-order the columns\n",
    "sample_order = [\n",
    "    \"pFDA-HG3-5x\",\n",
    "    \"pFDA-HG3-10x\",\n",
    "    \"pFDA-HG3-15x\",\n",
    "    \"pFDA-HG3-20x\",\n",
    "    \"pFDA-HG3-25x\",\n",
    "    \"pFDA-HG3-30x\",\n",
    "    \"pFDA-HG3-full\",\n",
    "    \"pFDA-HG4-full\",\n",
    "    \"pFDA-HG2-full\",\n",
    "    \"chemV2.2-HG2-full\",\n",
    "]\n",
    "runtime_df = runtime_df.set_index(\"sample\").reindex(sample_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the runtime metrics from seconds to hours\n",
    "for ridx, row in runtime_df.iterrows():\n",
    "    for cidx, val in row.items():\n",
    "        row[cidx] = val / 60 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some runtime metrics\n",
    "\n",
    "print(\"HG003 5x sample runtime:\")\n",
    "print(runtime_df.loc[\"pFDA-HG3-5x\"].sum())\n",
    "print(\"\")\n",
    "\n",
    "print(\"HG002 ChemV2.2 sample runtime:\")\n",
    "print(runtime_df.loc[\"chemV2.2-HG2-full\"].sum())\n",
    "print(\"\")\n",
    "\n",
    "print(\"ChemV2.2 first pass runtime:\")\n",
    "print(runtime_df[\n",
    "    ['RepeatStat', 'DNAscope-diploid', 'DNAModelApply-diploid']\n",
    "].loc[\"chemV2.2-HG2-full\"].sum())\n",
    "print(\"\")\n",
    "\n",
    "print(\"ChemV2.2 phasing runtime:\")\n",
    "print(runtime_df[\n",
    "    ['VariantPhaser', 'RepeatModel']\n",
    "].loc[\"chemV2.2-HG2-full\"].sum())\n",
    "print(\"\")\n",
    "\n",
    "print(\"ChemV2.2 second pass runtime:\")\n",
    "print(runtime_df[\n",
    "    ['DNAscope-hap1', 'DNAscope-hap2', 'DNAModelApply-hap1', 'DNAModelApply-hap2', \n",
    "     'DNAscopeHP-unphased', 'DNAModelApply-unphased']\n",
    "].loc[\"chemV2.2-HG2-full\"].sum())\n",
    "print(\"\")\n",
    "\n",
    "print(\"ChemV2.2 other runtime:\")\n",
    "print(runtime_df[\n",
    "    ['other_runtime']\n",
    "].loc[\"chemV2.2-HG2-full\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_df.to_csv(\n",
    "    \"../analysis/benchmark_runtime.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    float_format=\"%.5f\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_colors = [\n",
    "    (0.109, 0.494, 0.792), # RepeatStat\n",
    "    (0.047, 0.392, 0.513), # DNAscope-diploid\n",
    "    (0.062, 0.203, 0.776), # DNAModelApply-diploid\n",
    "    (0.964, 0.074, 0.156), # VariantPhaser\n",
    "    (0.964, 0.298, 0.074), # RepeatModel\n",
    "    (0.705, 0.972, 0.105), # DNAscope-hap1\n",
    "    (0.415, 0.972, 0.105), # DNAscope-hap2\n",
    "    (0.286, 0.874, 0.203), # DNAModelApply-hap1\n",
    "    (0.156, 0.784, 0.235), # DNAModelApply-hap2\n",
    "    (0.278, 0.980, 0.701), # DNAscopeHP-unphased\n",
    "    (0.380, 0.980, 0.823), # DNAModelApply-unphased\n",
    "    (0.898, 0.482, 0.043), # other_runtime\n",
    "]\n",
    "\n",
    "# Plot\n",
    "ax = runtime_df.plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    rot=30,\n",
    "    xlabel=\"Sample\",\n",
    "    ylabel=\"Runtime (hr)\",\n",
    "    color=runtime_colors,\n",
    ")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory usage by sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an empty dict with the columns\n",
    "memory_df = {\"sample\":[], \"max_vms\": []}\n",
    "\n",
    "# Add the data\n",
    "for sample, metrics in sample_runtime_metrics.items():\n",
    "    memory_df[\"sample\"].append(remap_names[sample])\n",
    "    memory_df[\"max_vms\"].append(metrics[\"max_vms\"] / 1024) # Memory from MB to GB\n",
    "    \n",
    "memory_df = pd.DataFrame(data=memory_df)\n",
    "\n",
    "# Skip samples not run under benchmark conditions\n",
    "memory_df = memory_df.loc[\n",
    "    (~memory_df[\"sample\"].isin(skip_samples))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_df.to_csv(\n",
    "    \"../analysis/benchmark_memory.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    float_format=\"%.5f\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(\n",
    "    x=\"sample\",\n",
    "    y=\"max_vms\",\n",
    "    data=memory_df,\n",
    "    order=sample_order,\n",
    ")\n",
    "ax.set(ylabel=\"Maximum Memory (GB)\", xlabel=\"Sample\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy vs. the pFDA Truth Challenge V2 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the full accuracy data frame\n",
    "regions_map = {\n",
    "    \"*\": \"All Benchmark Regions\",\n",
    "    \"GRCh38_MHC.bed.gz\": \"MHC\",\n",
    "    \"GRCh38_alllowmapandsegdupregions.bed.gz\": \"Difficult-to-Map Regions\",\n",
    "}\n",
    "pfda_samples = {\n",
    "    \"pFDA-truth_V2-PB-HG002--full--v4.2.1\": \"HG002\",\n",
    "    \"pFDA-truth_V2-PB-HG003--full--v4.2.1\": \"HG003\",\n",
    "    \"pFDA-truth_V2-PB-HG004--full--v4.2.1\": \"HG004\",\n",
    "    \"GH_pFDA_truth_V2--HG002--full--v4.2.1\": \"HG002\",\n",
    "    \"GH_pFDA_truth_V2--HG003--full--v4.2.1\": \"HG003\",\n",
    "    \"GH_pFDA_truth_V2--HG004--full--v4.2.1\": \"HG004\",\n",
    "}\n",
    "pfda_caller = {\n",
    "    \"pFDA-truth_V2-PB-HG002--full--v4.2.1\": \"DNAscope LongRead\",\n",
    "    \"pFDA-truth_V2-PB-HG003--full--v4.2.1\": \"DNAscope LongRead\",\n",
    "    \"pFDA-truth_V2-PB-HG004--full--v4.2.1\": \"DNAscope LongRead\",\n",
    "    \"GH_pFDA_truth_V2--HG002--full--v4.2.1\": \"pFDA Truth V2 Winning\",\n",
    "    \"GH_pFDA_truth_V2--HG003--full--v4.2.1\": \"pFDA Truth V2 Winning\",\n",
    "    \"GH_pFDA_truth_V2--HG004--full--v4.2.1\": \"pFDA Truth V2 Winning\",\n",
    "}\n",
    "\n",
    "pfda_acc_df = accuracy_metrics.loc[\n",
    "    (accuracy_metrics[\"Sample\"].isin(list(pfda_samples.keys()))) &\n",
    "    (accuracy_metrics[\"Subset\"].isin(list(regions_map.keys()))) &\n",
    "    (accuracy_metrics[\"Type\"] == \"SNP+INDEL\")\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfda_acc_df[\"Subset\"] = pfda_acc_df.apply(update_row_values(\"Subset\", regions_map), axis=1)\n",
    "pfda_acc_df[\"Caller\"] = pfda_acc_df.apply(update_row_values(\"Sample\", pfda_caller), axis=1)\n",
    "pfda_acc_df[\"Sample\"] = pfda_acc_df.apply(update_row_values(\"Sample\", pfda_samples), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfda_acc_df.drop(\n",
    "    [\"Type\", \"TOTAL.ERRORS\"],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy table\n",
    "pfda_acc_df.drop(\n",
    "    [\"Type\", \"TOTAL.ERRORS\"],\n",
    "    axis=1,\n",
    ").to_csv(\n",
    "    \"../analysis/pfda_acc_table.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    float_format=\"%.5f\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some accuracy metrics\n",
    "\n",
    "# pFDA average accuracy\n",
    "winning_mean = pfda_acc_df.loc[\n",
    "    (pfda_acc_df[\"Subset\"] == \"All Benchmark Regions\") &\n",
    "    (pfda_acc_df[\"Caller\"] == \"pFDA Truth V2 Winning\")\n",
    "][\"METRIC.F1_Score\"].mean()\n",
    "\n",
    "dnascope_mean = pfda_acc_df.loc[\n",
    "    (pfda_acc_df[\"Subset\"] == \"All Benchmark Regions\") &\n",
    "    (pfda_acc_df[\"Caller\"] == \"DNAscope LongRead\")\n",
    "][\"METRIC.F1_Score\"].mean()\n",
    "\n",
    "print(f\"DNAscope mean accuracy: {dnascope_mean}\")\n",
    "print(f\"Winning pipeline mean accuracy: {winning_mean}\")\n",
    "\n",
    "# pFDA mean errors by stratification\n",
    "stratifications = (\"All Benchmark Regions\", \"MHC\", \"Difficult-to-Map Regions\")\n",
    "callers = (\"pFDA Truth V2 Winning\", \"DNAscope LongRead\")\n",
    "for strat in stratifications:\n",
    "    mean_errors = []\n",
    "    for caller in callers:\n",
    "        mean_errors.append(\n",
    "            pfda_acc_df.loc[\n",
    "                (pfda_acc_df[\"Subset\"] == strat) &\n",
    "                (pfda_acc_df[\"Caller\"] == caller)\n",
    "            ][\"TOTAL.ERRORS\"].mean()\n",
    "        )\n",
    "    \n",
    "    mean_err_red = (mean_errors[0] - mean_errors[1]) / mean_errors[0]\n",
    "    print(f\"Mean error reduction across '{strat}' is: {mean_err_red}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent errors relative to DNAscope LongReads\n",
    "pfda_acc_barplot = pfda_acc_df.loc[\n",
    "    pfda_acc_df[\"Sample\"] == \"HG003\"\n",
    "].copy()\n",
    "\n",
    "dnascope_errs_dict = {}\n",
    "for idx, row in pfda_acc_barplot.loc[\n",
    "    pfda_acc_barplot[\"Caller\"] == \"DNAscope LongRead\"\n",
    "].iterrows():\n",
    "    dnascope_errs_dict[row[\"Subset\"]] = row[\"TOTAL.ERRORS\"]\n",
    "\n",
    "def relative_errors(row):\n",
    "    return row[\"TOTAL.ERRORS\"] / dnascope_errs_dict[row[\"Subset\"]] * 100\n",
    "    \n",
    "pfda_acc_barplot[\"Relative Errors\"] = pfda_acc_barplot.apply(relative_errors, axis=1)\n",
    "\n",
    "# plot\n",
    "ax = sns.barplot(x=\"Subset\", y=\"Relative Errors\", hue=\"Caller\", data=pfda_acc_barplot)\n",
    "ax.set(ylabel=\"% Errors Relative to DNAscope LongRead\")\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on serial downsamples of the pFDA HG003 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the full dataframe\n",
    "homopolymer_regions_map = {\n",
    "    \"*\": \"All Benchmark Regions\",\n",
    "    \"GRCh38_MHC.bed.gz\": \"MHC\",\n",
    "    \"GRCh38_alllowmapandsegdupregions.bed.gz\": \"Difficult-to-Map Regions\",\n",
    "    \"GRCh38_notinAllHomopolymers_gt6bp_imperfectgt10bp_slop5.bed.gz\": \"Not Long Homopolymer\",\n",
    "}\n",
    "sample_depth = {\n",
    "    \"pFDA-truth_V2-PB-HG003--full--v4.2.1\": 35,\n",
    "    \"pFDA-truth_V2-PB-HG003--857142--v4.2.1\": 30,\n",
    "    \"pFDA-truth_V2-PB-HG003--714285--v4.2.1\": 25,\n",
    "    \"pFDA-truth_V2-PB-HG003--571428--v4.2.1\": 20,\n",
    "    \"pFDA-truth_V2-PB-HG003--485714--v4.2.1\": 17,\n",
    "    \"pFDA-truth_V2-PB-HG003--428571--v4.2.1\": 15,\n",
    "    \"pFDA-truth_V2-PB-HG003--371429--v4.2.1\": 13,\n",
    "    \"pFDA-truth_V2-PB-HG003--314286--v4.2.1\": 11,\n",
    "    \"pFDA-truth_V2-PB-HG003--285714--v4.2.1\": 10,\n",
    "    \"pFDA-truth_V2-PB-HG003--142857--v4.2.1\": 5,\n",
    "}\n",
    "\n",
    "hg003_downsample_df = accuracy_metrics.loc[\n",
    "    (accuracy_metrics[\"Sample\"].isin(list(sample_depth.keys()))) &\n",
    "    (accuracy_metrics[\"Subset\"].isin(list(homopolymer_regions_map.keys())))\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg003_downsample_df[\"Depth\"] = hg003_downsample_df.apply(update_row_values(\"Sample\", sample_depth), axis=1)\n",
    "hg003_downsample_df[\"Subset\"] = hg003_downsample_df.apply(update_row_values(\"Subset\", homopolymer_regions_map), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg003_downsample_df.loc[\n",
    "    (\n",
    "        (hg003_downsample_df[\"Subset\"] == \"All Benchmark Regions\") |\n",
    "        (hg003_downsample_df[\"Subset\"] == \"Not Long Homopolymer\")\n",
    "    ) &\n",
    "    (hg003_downsample_df[\"Type\"] != \"SNP+INDEL\")\n",
    "].drop(\n",
    "    [\"TOTAL.ERRORS\", \"Sample\"],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg003_downsample_df.loc[\n",
    "    (\n",
    "        (hg003_downsample_df[\"Subset\"] == \"All Benchmark Regions\") |\n",
    "        (hg003_downsample_df[\"Subset\"] == \"Not Long Homopolymer\")\n",
    "    ) &\n",
    "    (hg003_downsample_df[\"Type\"] != \"SNP+INDEL\")\n",
    "].drop(\n",
    "    [\"TOTAL.ERRORS\", \"Sample\"],\n",
    "    axis=1,\n",
    ").to_csv(\n",
    "    \"../analysis/hg003_downsample_table.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    float_format=\"%.5f\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rows for Indel - no Homopolymer\n",
    "def update_var_type(row):\n",
    "    \"\"\" Update the variant type \"\"\"\n",
    "    if row[\"Type\"] == \"INDEL\" and row[\"Subset\"] == \"Not Long Homopolymer\":\n",
    "        return \"INDEL - not long Homopolymer\"\n",
    "    return row[\"Type\"]\n",
    "\n",
    "hg003_downsample_df[\"Type\"] = hg003_downsample_df.apply(update_var_type, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1-score by Depth\n",
    "ax = sns.lineplot(\n",
    "    y=\"METRIC.F1_Score\",\n",
    "    x=\"Depth\",\n",
    "    data=hg003_downsample_df.loc[\n",
    "        (\n",
    "            (hg003_downsample_df[\"Subset\"] == \"All Benchmark Regions\") &\n",
    "            (hg003_downsample_df[\"Type\"] != \"SNP+INDEL\")\n",
    "        ) | (\n",
    "            (hg003_downsample_df[\"Type\"] == \"INDEL - not long Homopolymer\")\n",
    "        )\n",
    "    ],\n",
    "    hue=\"Type\",\n",
    "    style=\"Type\",\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    ")\n",
    "ax.set(ylabel=\"F1-Score\")\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Phred(F1-score) by Depth\n",
    "hg003_downsample_df[\"Phred_F1\"] = hg003_downsample_df[\"METRIC.F1_Score\"].map(lambda x: -10 * math.log10(1 - x))\n",
    "\n",
    "ax = sns.lineplot(\n",
    "    y=\"Phred_F1\",\n",
    "    x=\"Depth\",\n",
    "    data=hg003_downsample_df.loc[\n",
    "        (\n",
    "            (hg003_downsample_df[\"Subset\"] == \"All Benchmark Regions\") &\n",
    "            (hg003_downsample_df[\"Type\"] != \"SNP+INDEL\")\n",
    "        ) | (\n",
    "            (hg003_downsample_df[\"Type\"] == \"INDEL - not long Homopolymer\")\n",
    "        )\n",
    "    ],\n",
    "    hue=\"Type\",\n",
    "    style=\"Type\",\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    ")\n",
    "ax.set(ylabel=\"Phred(F1-Score)\")\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on the CMRG benchmark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmrg_accuracy = accuracy_metrics.loc[\n",
    "    (accuracy_metrics[\"Sample\"] == \"pFDA-truth_V2-PB-HG002--full--CMRGv1.00\") &\n",
    "    (accuracy_metrics[\"Subset\"] == \"*\") &\n",
    "    (accuracy_metrics[\"Type\"] != \"SNP+INDEL\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmrg_errors = happy_to_errors(cmrg_accuracy)\n",
    "ax = sns.barplot(x=\"Variant Type\", y=\"Errors\", hue=\"Error\", data=cmrg_errors)\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmrg_metrics = happy_to_acc(cmrg_accuracy)\n",
    "ax = sns.barplot(x=\"Variant Type\", y=\"Performance\", hue=\"Metric\", data=cmrg_metrics)\n",
    "ax.set(ylim=(0.9, 1.003))\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmrg_accuracy.drop(\n",
    "    [\"Subset\", \"Sample\"],\n",
    "    axis=1,\n",
    ").to_csv(\n",
    "    \"../analysis/cmrg_table.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    float_format=\"%.5f\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on the Chemistry v2.2 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chem_accuracy = accuracy_metrics.loc[\n",
    "    (accuracy_metrics[\"Sample\"] == \"HG002-chemV2.2-sample--full--v4.2.1\") &\n",
    "    (accuracy_metrics[\"Subset\"] == \"*\") &\n",
    "    (accuracy_metrics[\"Type\"] != \"SNP+INDEL\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chem_errors = happy_to_errors(chem_accuracy)\n",
    "ax = sns.barplot(x=\"Variant Type\", y=\"Errors\", hue=\"Error\", data=chem_errors)\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chem_metrics = happy_to_acc(chem_accuracy)\n",
    "ax = sns.barplot(x=\"Variant Type\", y=\"Performance\", hue=\"Metric\", data=chem_metrics)\n",
    "ax.set(ylim=(0.98, 1.003))\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metrics.loc[\n",
    "    (accuracy_metrics[\"Sample\"] == \"HG002-chemV2.2-sample--full--v4.2.1\") &\n",
    "    (accuracy_metrics[\"Subset\"] == \"*\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metrics.loc[\n",
    "    (accuracy_metrics[\"Sample\"] == \"HG002-chemV2.2-sample--full--v4.2.1\") &\n",
    "    (accuracy_metrics[\"Subset\"] == \"*\")\n",
    "].copy().drop(\n",
    "    [\"Subset\", \"Sample\", \"TOTAL.ERRORS\"],\n",
    "    axis=1,\n",
    ").to_csv(\n",
    "    \"../analysis/chemistry-v2.2_table.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    float_format=\"%.5f\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on the Chem V2.2 sample vs. HG002 pFDA Truth V2 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chem_pfda_samples = {\n",
    "    \"HG002-chemV2.2-sample--full--v4.2.1\": \"HG002 - chemV2.2\",\n",
    "    \"pFDA-truth_V2-PB-HG002--full--v4.2.1\": \"HG002 - pFDA TruthV2\",\n",
    "}\n",
    "\n",
    "chem_accuracy = accuracy_metrics.loc[\n",
    "    (accuracy_metrics[\"Sample\"].isin(list(chem_pfda_samples.keys()))) &\n",
    "    (accuracy_metrics[\"Subset\"] == \"*\") &\n",
    "    (accuracy_metrics[\"Type\"] != \"SNP+INDEL\")\n",
    "].copy()\n",
    "\n",
    "chem_accuracy[\"Sample\"] = chem_accuracy.apply(update_row_values(\"Sample\", chem_pfda_samples), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"Type\", y=\"METRIC.F1_Score\", hue=\"Sample\", data=chem_accuracy)\n",
    "ax.set(ylim=(0.99, 1.001), xlabel=\"Variant Type\", ylabel=\"F-score\")\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
